{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Qué es el Aprendizaje por Refuerzo?\n",
    "\n",
    "\n",
    "Seguramente ya conocerás las 2 grandes áreas de aprendizaje tradicional del Machine Learning, el aprendizaje supervisado y el aprendizaje no supervisado. Parece difícil que aquí hubiera espacio para otras opciones; sin embargo sí la hay y es el Aprendizaje por refuerzo. En aprendizaje por refuerzo (ó Reinforcement Learning en inglés) no tenemos una “etiqueta de salida”, por lo que no es de tipo supervisado y si bien estos algoritmos aprenden por sí mismos, tampoco son de tipo no supervisado, en donde se intenta clasificar grupos teniendo en cuenta alguna distancia entre muestras.\n",
    "\n",
    "![foto1](img/areas-ml.png)\n",
    "\n",
    "![foto4](img/1_YIETknPBlQQBF40DJxL8QA.png)\n",
    "\n",
    "Si nos ponemos a pensar, los problemas de ML supervisados y no supervisados son específicos de un caso de negocio en particular, sea de clasificación ó predicción, están muy delimitados, por ejemplo, clasificar “perros ó gatos“, ó agrupar “k=5” clusters. En contraste, en el mundo real contamos con múltiples variables que por lo general se interrelacionan y que dependen de otros casos de negocio y dan lugar a escenarios más grandes en donde tomar decisiones. Para conducir un coche no basta una inteligencia que pueda detectar un semáforo en rojo, verde ó amarillo; tendremos muchísimos factores -todos a la vez- a los que prestar atención: a qué velocidad vamos, estamos ante una curva?, hay peatones?, es de noche y debemos encender las luces?.\n",
    "\n",
    "Una solución sería tener múltiples máquinas de ML supervisadas y que interactúan entre si -y esto no estaría mal- ó podemos cambiar el enfoque… Y ahí aparece el Reinforcement Learning (RL) como una alternativa, tal vez de las más ambiciosas en las que se intenta integrar el Machine Learning en el mundo real, sobre todo aplicado a robots y maquinaria industrial.\n",
    "\n",
    "El Reinforcement Learning entonces, intentará hacer aprender a la máquina basándose en un esquema de “premios y castigos” -cómo con el perro de Pablov- en un entorno en donde hay que tomar acciones y que está afectado por múltiples variables que cambian con el tiempo.\n",
    "\n",
    "![foto2](img/496302-1548589.jpg)\n",
    "\n",
    "\n",
    "## Diferencias con “los clásicos”\n",
    "\n",
    "En los modelos de Aprendizaje Supervisado (o no supervisado) como redes neuronales, árboles, knn, etc, se intenta “minimizar la función coste”, reducir el error.\n",
    "\n",
    "En cambio en el RL se intenta “maximizar la recompensa“. Y esto puede ser, a pesar de a veces cometer errores ó de no ser óptimos.\n",
    "\n",
    "Componentes del RL\n",
    "El Reinforcement Learning propone un nuevo enfoque para hacer que nuestra máquina aprenda, para ello, postula los siguientes 2 componentes:\n",
    "\n",
    "el Agente: será nuestro modelo que queremos entrenar y que aprenda a tomar decisiones.\n",
    "\n",
    "Ambiente: será el entorno en donde interactúa y “se mueve” el agente. El ambiente contiene las limitaciones y reglas posibles a cada momento.\n",
    "Entre ellos hay una relación que se retroalimenta y cuenta con los siguientes nexos:\n",
    "\n",
    "Acción: las posibles acciones que puede tomar en un momento determinado el Agente.\n",
    "\n",
    "Estado (del ambiente): son los indicadores del ambiente de cómo están los diversos elementos que lo componen en ese momento.\n",
    "\n",
    "Recompensas (ó castigos!): a raíz de cada acción tomada por el Agente, podremos obtener un premio ó una penalización que orientarán al Agente en si lo está haciendo bien ó mal.\n",
    "\n",
    "![foto3](img/premio-castigo.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Empecemos a trabajar Obreros.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cart Pole.\n",
    "\n",
    "![foto5](img/0-923627-95182.gif)\n",
    "\n",
    "Se trata de un péndulo que está en posición vertical y debemos mover el carro de abajo para que no caiga a ninguno de los lados.\n",
    "\n",
    "Este entorno es un problema clásico y como tal, ya está diseñado por nosotros en diferentes librerías. Nosotros usaremos OpenAI Gym. Se trata de una librería que provee de varios entornos interesantes para poner a prueba nuestros algoritmos, con representación gráfica de los mismos.\n",
    "\n",
    "Este entorno se caracteriza por tener cuatro entradas: posición del carro, velocidad del carro, ángulo del péndulo y velocidad angular del péndulo. Y dos acciones: acelerar a la izquierda y acelerar a la derecha. Las recompensas son 1 por cada acción tomada. El episodio finaliza cuando el ángulo del péndulo supera los 12 grados, el carro se aleja demasiado del centro y cuando el episodio alcanza los 200 pasos.\n",
    "\n",
    "Vamos a ver como funciona la interfaz Gym para que luego seamos capaces de crear nuestros propios entornos. Esta se compone de:\n",
    "\n",
    "Una clase que hereda de **gym.Env**\n",
    "\n",
    "Un constructor donde inicializamos, como mínimo, self.action_space, self.observation_space y opcionalmente self.reward_range y parámetros internos del entorno nuestro. Estas dos primeras deben ser de algún subtipo **gym.spaces**. Los más normales son **Discrete**, que es básicamente un valor activado sobre N posibles y Box, que es una matriz de características (podemos especificar la forma para que sea unidimensional, o que tenga más dimensiones), que contiene números. \n",
    "Es muy habitual que las observaciones sean de tipo Box, ya que se miden varias características a la vez, con números decimales y **Discrete** sea para las acciones ya que tomamos una acción de N posibles. Pero otras combinaciones son posibles y existen más tipos de **gym.spaces**. Si vamos a introducir características diferentes dentro del mismo Box es muy conveniente normalizar los datos entre -1 y 1. self.reward_range, al contrario, es una tupla en la que se especifica el valor máximo y el mínimo que pueden alcanzar las recompensas.\n",
    "\n",
    "Un método **reset**, que reinicia el entorno y devuelve una observación inicial. Estas observaciones, deberán devolverse con NumPy si son de tipo Box y coincidir con lo declarado en el constructor en cuanto a forma y tipos.\n",
    "\n",
    "Un método **step** que toma una acción y devuelve una tupla con cuatro valores: nueva observación, recompensa, un boolean de si el episodio ha acabado ya o no y un diccionario de información extra (no usado por los algoritmos).\n",
    "\n",
    "Opcionalmente pueden llevar instrucciones sobre como renderizar el entorno y así ver en vivo o en vídeo el funcionamiento de los algoritmos. Principalmente se controla a través del método **render**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[cartpole](https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space Discrete(2)\n",
      "State Space Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n"
     ]
    }
   ],
   "source": [
    "env=gym.make('CartPole-v0') # Crea el entorno de juego correspondiente\n",
    "\n",
    "env.seed(1) #Opcional, establezca un número aleatorio para que el proceso se pueda repetir\n",
    "\n",
    "env=env.unwrapped #Opcional, agregue restricciones al medio ambiente, beneficioso para la capacitación\n",
    "\n",
    "# ---------------------- Espacio de acción y espacio de estado --------------------- #\n",
    "\n",
    "print(\"Action Space {}\".format(env.action_space)) # Posibles Movimientos\n",
    "\n",
    "print(\"State Space {}\".format(env.observation_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "for _ in range(1000):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bien ahora tenemos un carrito idiota que no hace nada solo moverse..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si queremos tomar acciones que son mejores que las aleatorias en cada paso, entonces sería bueno comprender realmente cómo nuestras acciones afectan el medio ambiente. A\n",
    "La función de paso del entorno devolverá la información que necesitamos. La función paso devuelve cuatro valores, la siguiente es la información específica:\n",
    "\n",
    "Observación (objet): un objeto relacionado con el entorno describe el entorno que observa, como la información de píxeles de la cámara, el ángulo y la velocidad angular del robot y el estado del tablero en un juego de mesa.\n",
    "\n",
    "recompensa (float): La suma de todas las recompensas obtenidas de acciones anteriores. Los diferentes entornos tienen diferentes métodos de cálculo, pero el objetivo siempre ha sido aumentar tus recompensas totales.\n",
    "\n",
    "done (booleano): para determinar si es el momento de restablecer el entorno, la mayoría de las tareas se dividen en episodios claramente definidos, y done es True para indicar que el episodio ha terminado.\n",
    "\n",
    "info (dict): información de diagnóstico para depuración. A veces también es útil para aprender (por ejemplo, puede contener la probabilidad original después del último cambio de estado del entorno). Pero la evaluación formal del agente no permite el uso de esta información para el aprendizaje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.04022996 -0.02607133  0.04099473  0.04875433]\n",
      "[-0.04075139 -0.2217564   0.04196982  0.3540844 ]\n",
      "[-0.04518652 -0.4174492   0.04905151  0.65970063]\n",
      "[-0.0535355  -0.223043    0.06224552  0.38285738]\n",
      "[-0.05799636 -0.41899094  0.06990267  0.6944985 ]\n",
      "[-0.06637618 -0.22490466  0.08379264  0.4246147 ]\n",
      "[-0.07087427 -0.03106343  0.09228493  0.15947923]\n",
      "[-0.07149554 -0.2273771   0.09547452  0.47978997]\n",
      "[-0.07604308 -0.03372345  0.10507032  0.2186593 ]\n",
      "[-0.07671756 -0.2301782   0.1094435   0.54255015]\n",
      "[-0.08132111 -0.03675081  0.1202945   0.28625718]\n",
      "[-0.08205613 -0.2333646   0.12601966  0.6143279 ]\n",
      "[-0.08672342 -0.4300012   0.13830622  0.94389564]\n",
      "[-0.09532345 -0.23698618  0.15718412  0.6976703 ]\n",
      "[-0.10006317 -0.43389803  0.17113753  1.0354189 ]\n",
      "[-0.10874113 -0.24141316  0.19184591  0.80097854]\n",
      "[-0.11356939 -0.43857563  0.20786548  1.1473539 ]\n",
      "Episode finished after 17 timesteps\n",
      "[-0.02574172 -0.03604378 -0.01655194  0.00040553]\n",
      "[-0.0264626   0.1593116  -0.01654383 -0.29745343]\n",
      "[-0.02327637 -0.03557067 -0.0224929  -0.01003372]\n",
      "[-0.02398778 -0.23036294 -0.02269357  0.2754684 ]\n",
      "[-0.02859504 -0.42515388 -0.01718421  0.56090826]\n",
      "[-0.03709812 -0.6200305  -0.00596604  0.84812814]\n",
      "[-0.04949873 -0.81507057  0.01099652  1.138929  ]\n",
      "[-0.06580014 -1.0103346   0.0337751   1.4350402 ]\n",
      "[-0.08600684 -0.81564504  0.06247591  1.1531005 ]\n",
      "[-0.10231973 -0.6213912   0.08553792  0.88064414]\n",
      "[-0.11474755 -0.8175645   0.1031508   1.1989458 ]\n",
      "[-0.13109885 -1.0138586   0.12712972  1.5220945 ]\n",
      "[-0.15137602 -1.2102666   0.1575716   1.8516045 ]\n",
      "[-0.17558135 -1.4067323   0.1946037   2.1887865 ]\n",
      "Episode finished after 14 timesteps\n",
      "[ 0.01398089  0.01504228  0.00590911 -0.02005313]\n",
      "[ 0.01428174 -0.18016392  0.00550805  0.27448833]\n",
      "[ 0.01067846 -0.37536404  0.01099782  0.5689034 ]\n",
      "[ 0.00317118 -0.18039803  0.02237589  0.2797054 ]\n",
      "[-4.3678374e-04 -3.7583190e-01  2.7969994e-02  5.7936078e-01]\n",
      "[-0.00795342 -0.5713344   0.03955721  0.880722  ]\n",
      "[-0.01938011 -0.7669708   0.05717165  1.1855737 ]\n",
      "[-0.03471953 -0.57263505  0.08088312  0.9113457 ]\n",
      "[-0.04617223 -0.3786952   0.09911004  0.6451407 ]\n",
      "[-0.05374613 -0.18508387  0.11201285  0.38523957]\n",
      "[-0.05744781 -0.381603    0.11971764  0.7110339 ]\n",
      "[-0.06507987 -0.5781615   0.13393833  1.0388741 ]\n",
      "[-0.07664309 -0.38504887  0.1547158   0.7910599 ]\n",
      "[-0.08434407 -0.58191824  0.17053701  1.1281419 ]\n",
      "[-0.09598244 -0.7788131   0.19309984  1.4690951 ]\n",
      "Episode finished after 15 timesteps\n",
      "[-0.00466507  0.02935217  0.04329036  0.024508  ]\n",
      "[-0.00407803 -0.166363    0.04378052  0.33052912]\n",
      "[-0.00740529  0.02810928  0.0503911   0.05196756]\n",
      "[-0.0068431  -0.16769765  0.05143045  0.36011386]\n",
      "[-0.01019706  0.02665693  0.05863273  0.08408134]\n",
      "[-0.00966392 -0.16925435  0.06031436  0.3946714 ]\n",
      "[-0.013049   -0.36517793  0.06820779  0.7057441 ]\n",
      "[-0.02035256 -0.56117535  0.08232266  1.0190941 ]\n",
      "[-0.03157607 -0.7572922   0.10270455  1.3364488 ]\n",
      "[-0.04672191 -0.9535472   0.12943353  1.659422  ]\n",
      "[-0.06579286 -1.1499186   0.16262196  1.9894625 ]\n",
      "[-0.08879123 -1.3463299   0.20241122  2.327793  ]\n",
      "Episode finished after 12 timesteps\n",
      "[0.01429291 0.01675391 0.03075361 0.02071383]\n",
      "[ 0.01462799 -0.17879528  0.03116788  0.32293904]\n",
      "[0.01105208 0.01586931 0.03762667 0.04024586]\n",
      "[ 0.01136947  0.21043207  0.03843158 -0.24033208]\n",
      "[ 0.01557811  0.40498453  0.03362494 -0.5206492 ]\n",
      "[ 0.0236778   0.2094058   0.02321196 -0.21756303]\n",
      "[0.02786591 0.01395984 0.0188607  0.08235062]\n",
      "[ 0.02814511  0.20880641  0.02050771 -0.20432255]\n",
      "[ 0.03232124  0.40362918  0.01642126 -0.49046642]\n",
      "[ 0.04039383  0.5985157   0.00661193 -0.7779291 ]\n",
      "[ 0.05236414  0.7935461  -0.00894665 -1.0685245 ]\n",
      "[ 0.06823506  0.59854364 -0.03031714 -0.7786627 ]\n",
      "[ 0.08020593  0.794069   -0.0458904  -1.0807279 ]\n",
      "[ 0.09608731  0.98976576 -0.06750496 -1.387451  ]\n",
      "[ 0.11588263  0.795547   -0.09525397 -1.1166178 ]\n",
      "[ 0.13179357  0.60179543 -0.11758633 -0.85527027]\n",
      "[ 0.14382948  0.7983066  -0.13469173 -1.1824924 ]\n",
      "[ 0.15979561  0.9948944  -0.15834159 -1.5141829 ]\n",
      "[ 0.17969349  1.1915389  -0.18862525 -1.851818  ]\n",
      "Episode finished after 19 timesteps\n",
      "[ 0.02578619 -0.00074554  0.00241298  0.03115522]\n",
      "[ 0.02577128  0.19434172  0.00303609 -0.2607654 ]\n",
      "[ 0.02965811  0.3894202  -0.00217922 -0.55248916]\n",
      "[ 0.03744652  0.19432892 -0.01322901 -0.26049364]\n",
      "[ 0.0413331   0.3896372  -0.01843888 -0.55731964]\n",
      "[ 0.04912584  0.5850131  -0.02958527 -0.8557544 ]\n",
      "[ 0.0608261   0.78052545 -0.04670036 -1.1575913 ]\n",
      "[ 0.07643661  0.5860422  -0.06985219 -0.8799095 ]\n",
      "[ 0.08815746  0.7820401  -0.08745038 -1.1937093 ]\n",
      "[ 0.10379826  0.978179   -0.11132456 -1.512471  ]\n",
      "[ 0.12336184  1.1744591  -0.14157398 -1.8377305 ]\n",
      "[ 0.14685102  1.3708334  -0.17832859 -2.1708288 ]\n",
      "Episode finished after 12 timesteps\n",
      "[ 0.01799173  0.03472098 -0.04370931 -0.02684962]\n",
      "[ 0.01868615 -0.15974778 -0.0442463   0.25172848]\n",
      "[ 0.01549119  0.03597715 -0.03921174 -0.05457576]\n",
      "[ 0.01621074 -0.15856126 -0.04030325  0.22548226]\n",
      "[ 0.01303951 -0.3530847  -0.03579361  0.50518453]\n",
      "[ 0.00597782 -0.15747711 -0.02568991  0.20144   ]\n",
      "[ 0.00282827 -0.3522224  -0.02166112  0.48590955]\n",
      "[-0.00421617 -0.5470321  -0.01194292  0.7716877 ]\n",
      "[-0.01515682 -0.7419877   0.00349083  1.0605891 ]\n",
      "[-0.02999657 -0.9371557   0.02470261  1.3543656 ]\n",
      "[-0.04873968 -1.1325788   0.05178992  1.6546727 ]\n",
      "[-0.07139126 -1.3282655   0.08488338  1.9630287 ]\n",
      "[-0.09795658 -1.5241773   0.12414395  2.2807639 ]\n",
      "[-0.12844011 -1.3304064   0.16975923  2.0287492 ]\n",
      "Episode finished after 14 timesteps\n",
      "[ 0.01630063 -0.00861989  0.03352219  0.04147297]\n",
      "[ 0.01612823 -0.20420611  0.03435165  0.34454113]\n",
      "[ 0.01204411 -0.00958924  0.04124248  0.06288547]\n",
      "[ 0.01185232  0.18491787  0.04250018 -0.21650524]\n",
      "[ 0.01555068 -0.01078507  0.03817008  0.08927538]\n",
      "[ 0.01533498 -0.20643276  0.03995559  0.39375243]\n",
      "[ 0.01120632 -0.01189989  0.04783064  0.11392984]\n",
      "[ 0.01096833  0.18250524  0.05010923 -0.1632875 ]\n",
      "[ 0.01461843  0.37687537  0.04684348 -0.4397508 ]\n",
      "[ 0.02215594  0.18112284  0.03804847 -0.13267714]\n",
      "[ 0.0257784  -0.0145229   0.03539493  0.1717626 ]\n",
      "[ 0.02548794 -0.21013309  0.03883018  0.47539806]\n",
      "[ 0.02128527 -0.40578124  0.04833814  0.78006226]\n",
      "[ 0.01316965 -0.21135597  0.06393939  0.5029709 ]\n",
      "[ 0.00894253 -0.01719074  0.0739988   0.23110355]\n",
      "[ 0.00859872  0.1768001   0.07862087 -0.03735057]\n",
      "[ 0.01213472 -0.01935611  0.07787386  0.27906558]\n",
      "[0.0117476  0.17457356 0.08345517 0.01192445]\n",
      "[ 0.01523907  0.36840564  0.08369366 -0.25330442]\n",
      "[0.02260718 0.17219457 0.07862757 0.06455922]\n",
      "[ 0.02605107 -0.0239615   0.07991876  0.38097697]\n",
      "[ 0.02557184 -0.22012195  0.08753829  0.6977496 ]\n",
      "[ 0.0211694  -0.02631589  0.10149328  0.43385702]\n",
      "[0.02064309 0.1672337  0.11017043 0.17481449]\n",
      "[ 0.02398776  0.36062062  0.11366672 -0.08118262]\n",
      "[ 0.03120017  0.5539452   0.11204307 -0.33595204]\n",
      "[ 0.04227908  0.74730915  0.10532402 -0.5913075 ]\n",
      "[ 0.05722526  0.9408111   0.09349787 -0.849044  ]\n",
      "[ 0.07604148  0.7445469   0.07651699 -0.5284852 ]\n",
      "[ 0.09093241  0.5484365   0.06594729 -0.21270633]\n",
      "[0.10190115 0.35243672 0.06169317 0.10002875]\n",
      "[ 0.10894988  0.54662275  0.06369374 -0.17256986]\n",
      "[ 0.11988234  0.74077797  0.06024234 -0.44449887]\n",
      "[ 0.1346979   0.54485774  0.05135236 -0.13345106]\n",
      "[0.14559506 0.34903926 0.04868335 0.17498042]\n",
      "[0.15257584 0.1532556  0.05218295 0.4826152 ]\n",
      "[ 0.15564094 -0.04256254  0.06183526  0.7912774 ]\n",
      "[ 0.1547897  -0.23847656  0.07766081  1.1027548 ]\n",
      "[ 0.15002017 -0.04445731  0.0997159   0.83541244]\n",
      "[ 0.14913101 -0.24078971  0.11642415  1.157716  ]\n",
      "[ 0.14431523 -0.43722048  0.13957848  1.484519  ]\n",
      "[ 0.13557081 -0.63374084  0.16926885  1.8173356 ]\n",
      "[ 0.122896   -0.8302933   0.20561557  2.1574783 ]\n",
      "Episode finished after 43 timesteps\n",
      "[ 0.02868584  0.00490524 -0.03972737 -0.01066414]\n",
      "[ 0.02878395 -0.18962511 -0.03994065  0.26922438]\n",
      "[ 0.02499145  0.00604339 -0.03455616 -0.03578372]\n",
      "[ 0.02511231 -0.18856642 -0.03527184  0.24579923]\n",
      "[ 0.02134099  0.00704108 -0.03035585 -0.0577973 ]\n",
      "[ 0.02148181 -0.18763277 -0.0315118   0.2251557 ]\n",
      "[ 0.01772915  0.00792505 -0.02700868 -0.07729834]\n",
      "[ 0.01788765  0.20342357 -0.02855465 -0.37837875]\n",
      "[ 0.02195613  0.00871853 -0.03612223 -0.09483419]\n",
      "[ 0.02213049  0.2043391  -0.03801891 -0.39869124]\n",
      "[ 0.02621728  0.00977654 -0.04599274 -0.11823317]\n",
      "[ 0.02641281 -0.18465729 -0.0483574   0.15959206]\n",
      "[ 0.02271966  0.01112242 -0.04516556 -0.14794575]\n",
      "[ 0.02294211 -0.18332464 -0.04812447  0.13015324]\n",
      "[ 0.01927562 -0.37772536 -0.04552141  0.40727356]\n",
      "[ 0.01172111 -0.57217324 -0.03737593  0.68526465]\n",
      "[ 2.7764606e-04 -3.7655288e-01 -2.3670644e-02  3.8105303e-01]\n",
      "[-0.00725341 -0.18110296 -0.01604958  0.08100174]\n",
      "[-0.01087547  0.01424535 -0.01442955 -0.21670134]\n",
      "[-0.01059056 -0.18066739 -0.01876358  0.07139517]\n",
      "[-0.01420391 -0.37551537 -0.01733567  0.35809955]\n",
      "[-0.02171422 -0.57038665 -0.01017368  0.64526606]\n",
      "[-0.03312195 -0.76536536  0.00273164  0.934728  ]\n",
      "[-0.04842926 -0.960524    0.0214262   1.228268  ]\n",
      "[-0.06763974 -0.7656843   0.04599156  0.94237435]\n",
      "[-0.08295342 -0.9613948   0.06483905  1.2491465 ]\n",
      "[-0.10218132 -1.1572852   0.08982198  1.5614139 ]\n",
      "[-0.12532702 -1.3533596   0.12105025  1.880712  ]\n",
      "[-0.15239422 -1.1597466   0.1586645   1.6279198 ]\n",
      "[-0.17558916 -1.3563385   0.19122289  1.9655553 ]\n",
      "Episode finished after 30 timesteps\n",
      "[ 0.04364241  0.03990253  0.04170968 -0.00906715]\n",
      "[ 0.04444046 -0.15579201  0.04152833  0.29647845]\n",
      "[0.04132462 0.03871408 0.0474579  0.01717668]\n",
      "[ 0.0420989   0.23312443  0.04780143 -0.26016313]\n",
      "[0.04676139 0.03735385 0.04259817 0.04720559]\n",
      "[ 0.04750847  0.23183991  0.04354228 -0.23173878]\n",
      "[ 0.05214527  0.4263135   0.03890751 -0.5103753 ]\n",
      "[ 0.06067153  0.62086636  0.0287     -0.79054785]\n",
      "[ 0.07308886  0.8155827   0.01288905 -1.0740654 ]\n",
      "[ 0.08940051  0.62029284 -0.00859226 -0.7773656 ]\n",
      "[ 0.10180637  0.81553185 -0.02413957 -1.0727395 ]\n",
      "[ 0.11811701  0.6207372  -0.04559436 -0.7877289 ]\n",
      "[ 0.13053176  0.81645477 -0.06134894 -1.0944003 ]\n",
      "[ 0.14686085  1.0123289  -0.08323695 -1.405684  ]\n",
      "[ 0.16710743  0.81833297 -0.11135063 -1.140141  ]\n",
      "[ 0.18347408  0.6248287  -0.13415344 -0.8843517 ]\n",
      "[ 0.19597065  0.43175837 -0.15184048 -0.63667125]\n",
      "[ 0.20460583  0.2390435  -0.16457391 -0.39539468]\n",
      "[ 0.20938669  0.43607137 -0.1724818  -0.73510975]\n",
      "[ 0.21810812  0.633103   -0.18718399 -1.0767305 ]\n",
      "[ 0.23077019  0.4408799  -0.2087186  -0.8481437 ]\n",
      "Episode finished after 21 timesteps\n",
      "[-0.02313345  0.00767393 -0.01615024 -0.00380077]\n",
      "[-0.02297997  0.20302373 -0.01622625 -0.30153522]\n",
      "[-0.01891949  0.00813676 -0.02225696 -0.01401356]\n",
      "[-0.01875676  0.20357072 -0.02253723 -0.3136349 ]\n",
      "[-0.01468534  0.39900637 -0.02880993 -0.6133393 ]\n",
      "[-0.00670522  0.2042986  -0.04107671 -0.3298678 ]\n",
      "[-0.00261924  0.00978473 -0.04767407 -0.05041627]\n",
      "[-0.00242355 -0.18462235 -0.0486824   0.22685216]\n",
      "[-0.006116   -0.37901598 -0.04414535  0.5037902 ]\n",
      "[-0.01369632 -0.18330057 -0.03406955  0.19752854]\n",
      "[-0.01736233 -0.37791908 -0.03011898  0.47927237]\n",
      "[-0.02492071 -0.18238515 -0.02053353  0.17725088]\n",
      "[-0.02856841 -0.3772073  -0.01698851  0.4633861 ]\n",
      "[-0.03611256 -0.57208514 -0.00772079  0.75066626]\n",
      "[-0.04755426 -0.76709974  0.00729253  1.0409095 ]\n",
      "[-0.06289626 -0.9623178   0.02811073  1.3358729 ]\n",
      "[-0.08214261 -0.76756114  0.05482818  1.0521168 ]\n",
      "[-0.09749383 -0.57320744  0.07587051  0.77713567]\n",
      "[-0.10895798 -0.3792063   0.09141323  0.50925565]\n",
      "[-0.11654211 -0.57548916  0.10159834  0.8292887 ]\n",
      "[-0.12805189 -0.77184236  0.11818412  1.1521183 ]\n",
      "[-0.14348873 -0.57844365  0.14122649  0.8987091 ]\n",
      "[-0.15505761 -0.38548908  0.15920067  0.6535415 ]\n",
      "[-0.1627674  -0.5824276   0.17227149  0.9918212 ]\n",
      "[-0.17441595 -0.38997698  0.19210792  0.75781596]\n",
      "[-0.18221548 -0.19794807  0.20726424  0.5312044 ]\n",
      "Episode finished after 26 timesteps\n",
      "[0.02691469 0.02887109 0.0122744  0.02588815]\n",
      "[ 0.02749212 -0.1664247   0.01279216  0.32241836]\n",
      "[0.02416362 0.02851276 0.01924053 0.03379687]\n",
      "[ 0.02473387 -0.16687974  0.01991646  0.33248776]\n",
      "[0.02139628 0.02795315 0.02656622 0.04615143]\n",
      "[ 0.02195534  0.22268428  0.02748925 -0.23803265]\n",
      "[ 0.02640903  0.41740295  0.02272859 -0.5219195 ]\n",
      "[ 0.03475709  0.22196858  0.0122902  -0.22216201]\n",
      "[ 0.03919646  0.41691273  0.00784696 -0.51094294]\n",
      "[ 0.04753472  0.22168112 -0.00237189 -0.21579754]\n",
      "[ 0.05196834  0.4168369  -0.00668784 -0.5092277 ]\n",
      "[ 0.06030507  0.61205244 -0.0168724  -0.8040107 ]\n",
      "[ 0.07254612  0.41716582 -0.03295261 -0.5166826 ]\n",
      "[ 0.08088944  0.6127359  -0.04328626 -0.8195648 ]\n",
      "[ 0.09314416  0.8084227  -0.05967756 -1.1255423 ]\n",
      "[ 0.10931261  0.61413145 -0.08218841 -0.85215896]\n",
      "[ 0.12159524  0.42022035 -0.09923159 -0.58641034]\n",
      "[ 0.12999965  0.61658186 -0.11095979 -0.9086299 ]\n",
      "[ 0.14233129  0.81301665 -0.12913239 -1.2340254 ]\n",
      "[ 0.15859161  0.61976933 -0.1538129  -0.9844273 ]\n",
      "[ 0.17098701  0.81657964 -0.17350145 -1.3212004 ]\n",
      "[ 0.1873186   1.013417   -0.19992545 -1.6627774 ]\n",
      "Episode finished after 22 timesteps\n",
      "[-0.03218526 -0.02067065  0.0143567  -0.03037118]\n",
      "[-0.03259868  0.17424251  0.01374928 -0.31849006]\n",
      "[-0.02911383 -0.02107254  0.00737948 -0.02150301]\n",
      "[-0.02953528 -0.21629955  0.00694942  0.27349904]\n",
      "[-0.03386127 -0.41151994  0.0124194   0.5683657 ]\n",
      "[-0.04209167 -0.21657439  0.02378671  0.27962112]\n",
      "[-0.04642316 -0.02179969  0.02937914 -0.00546563]\n",
      "[-0.04685915 -0.2173304   0.02926983  0.29634008]\n",
      "[-0.05120576 -0.02263767  0.03519662  0.01303018]\n",
      "[-0.05165851 -0.21824625  0.03545723  0.316607  ]\n",
      "[-0.05602344 -0.0236468   0.04178937  0.03531354]\n",
      "[-0.05649637 -0.21934234  0.04249564  0.34088308]\n",
      "[-0.06088322 -0.02484997  0.0493133   0.06189786]\n",
      "[-0.06138022 -0.220643    0.05055126  0.3697226 ]\n",
      "[-0.06579307 -0.02627437  0.05794571  0.09339785]\n",
      "[-0.06631856  0.16797124  0.05981367 -0.18045503]\n",
      "[-0.06295914  0.36218855  0.05620457 -0.45368525]\n",
      "[-0.05571537  0.55647254  0.04713086 -0.72813565]\n",
      "[-0.04458592  0.36073184  0.03256815 -0.4209993 ]\n",
      "[-0.03737128  0.16516393  0.02414816 -0.11822966]\n",
      "[-0.034068   -0.03029553  0.02178357  0.18197294]\n",
      "[-0.03467391  0.16450804  0.02542303 -0.10375924]\n",
      "[-0.03138375 -0.03096884  0.02334785  0.1968349 ]\n",
      "[-0.03200313  0.1638115   0.02728454 -0.08839235]\n",
      "[-0.0287269  -0.03169069  0.0255167   0.21277247]\n",
      "[-0.02936071 -0.227168    0.02977215  0.51339406]\n",
      "[-0.03390408 -0.42269635  0.04004003  0.81530833]\n",
      "[-0.042358   -0.618343    0.05634619  1.1203117 ]\n",
      "[-0.05472486 -0.81415695  0.07875243  1.4301234 ]\n",
      "[-0.071008   -0.62009066  0.1073549   1.1630555 ]\n",
      "[-0.08340982 -0.42651772  0.13061601  0.90586954]\n",
      "[-0.09194016 -0.6231434   0.1487334   1.2365876 ]\n",
      "[-0.10440303 -0.81982994  0.17346515  1.5719268 ]\n",
      "[-0.12079963 -0.62715006  0.20490369  1.3379878 ]\n",
      "Episode finished after 34 timesteps\n",
      "[ 0.02806888 -0.01579189 -0.00844405 -0.01692937]\n",
      "[ 0.02775304 -0.21079174 -0.00878263  0.27307746]\n",
      "[ 0.02353721 -0.4057873  -0.00332109  0.56297743]\n",
      "[ 0.01542146 -0.21061888  0.00793846  0.26925004]\n",
      "[ 0.01120909 -0.01561111  0.01332346 -0.02091852]\n",
      "[ 0.01089686 -0.21092157  0.01290509  0.27593815]\n",
      "[ 0.00667843 -0.0159861   0.01842386 -0.01264672]\n",
      "[ 0.00635871 -0.21136735  0.01817092  0.28579175]\n",
      "[ 0.00213136 -0.01650921  0.02388676 -0.00110527]\n",
      "[ 0.00180118  0.17826217  0.02386465 -0.28615695]\n",
      "[ 0.00536642 -0.01719185  0.01814151  0.01395614]\n",
      "[ 0.00502259 -0.2125692   0.01842063  0.31230727]\n",
      "[ 0.0007712  -0.40794867  0.02466678  0.6107421 ]\n",
      "[-0.00738777 -0.21318004  0.03688162  0.3259293 ]\n",
      "[-0.01165137 -0.40880716  0.04340021  0.6300112 ]\n",
      "[-0.01982751 -0.604507    0.05600043  0.93604016]\n",
      "[-0.03191765 -0.4101832   0.07472124  0.66146696]\n",
      "[-0.04012132 -0.60626096  0.08795057  0.9767105 ]\n",
      "[-0.05224654 -0.80244523  0.10748479  1.2956736 ]\n",
      "[-0.06829544 -0.9987558   0.13339826  1.6199799 ]\n",
      "[-0.08827056 -0.8054341   0.16579786  1.3716764 ]\n",
      "[-0.10437924 -0.6127277   0.19323139  1.1351024 ]\n",
      "Episode finished after 22 timesteps\n",
      "[0.02170442 0.04908663 0.02052992 0.04058659]\n",
      "[ 0.02268616  0.24390826  0.02134165 -0.24554889]\n",
      "[ 0.02756432  0.438719    0.01643068 -0.53142446]\n",
      "[ 0.0363387   0.24336983  0.00580219 -0.23360988]\n",
      "[ 0.0412061   0.4384084   0.00112999 -0.524457  ]\n",
      "[ 0.04997427  0.24327056 -0.00935915 -0.2314182 ]\n",
      "[ 0.05483968  0.438525   -0.01398752 -0.5270386 ]\n",
      "[ 0.06361017  0.6338409  -0.02452829 -0.824096  ]\n",
      "[ 0.07628699  0.43906292 -0.0410102  -0.5392275 ]\n",
      "[ 0.08506826  0.24454077 -0.05179476 -0.25974318]\n",
      "[ 0.08995907  0.050195   -0.05698962  0.01616365]\n",
      "[ 0.09096297 -0.1440653  -0.05666635  0.29033518]\n",
      "[ 0.08808167  0.05181696 -0.05085965 -0.01966759]\n",
      "[ 0.089118    0.24763    -0.05125299 -0.32795367]\n",
      "[ 0.09407061  0.05327376 -0.05781207 -0.05186389]\n",
      "[ 0.09513608  0.24917497 -0.05884935 -0.36221206]\n",
      "[ 0.10011958  0.05493669 -0.06609359 -0.08865075]\n",
      "[ 0.10121831 -0.1391787  -0.0678666   0.1824692 ]\n",
      "[ 0.09843474 -0.3332672  -0.06421722  0.45299432]\n",
      "[ 0.09176939 -0.13729872 -0.05515733  0.1407806 ]\n",
      "[ 0.08902342 -0.3315891  -0.05234172  0.41556492]\n",
      "[ 0.08239163 -0.1357659  -0.04403042  0.10685094]\n",
      "[ 0.07967632 -0.33023012 -0.0418934   0.38532382]\n",
      "[ 0.07307172 -0.52473307 -0.03418693  0.6645092 ]\n",
      "[ 0.06257705 -0.7193632  -0.02089674  0.9462348 ]\n",
      "[ 0.04818979 -0.52396613 -0.00197205  0.64706004]\n",
      "[ 0.03771047 -0.71906054  0.01096915  0.9391213 ]\n",
      "[ 0.02332926 -0.91432863  0.02975158  1.2352307 ]\n",
      "[ 0.00504268 -1.1098201   0.05445619  1.5370837 ]\n",
      "[-0.01715372 -0.91539425  0.08519787  1.2618796 ]\n",
      "[-0.0354616  -0.72145885  0.11043546  0.9970494 ]\n",
      "[-0.04989078 -0.52797294  0.13037644  0.74099064]\n",
      "[-0.06045024 -0.7246307   0.14519626  1.0716959 ]\n",
      "[-0.07494285 -0.9213424   0.16663018  1.4061965 ]\n",
      "[-0.0933697  -1.1180941   0.19475411  1.7459991 ]\n",
      "Episode finished after 35 timesteps\n",
      "[-0.01523282  0.03900718 -0.01738005 -0.04613311]\n",
      "[-0.01445267 -0.15586129 -0.01830271  0.24101597]\n",
      "[-0.0175699  -0.35071707 -0.01348239  0.52787   ]\n",
      "[-0.02458424 -0.15540805 -0.00292499  0.23096938]\n",
      "[-0.0276924  -0.3504881   0.0016944   0.52272826]\n",
      "[-0.03470216 -0.54563385  0.01214896  0.8159446 ]\n",
      "[-0.04561484 -0.35068032  0.02846785  0.5271076 ]\n",
      "[-0.05262845 -0.15597025  0.03901     0.24352926]\n",
      "[-0.05574785  0.03857341  0.04388059 -0.03659818]\n",
      "[-0.05497639 -0.15714943  0.04314863  0.2696002 ]\n",
      "[-0.05811937 -0.35285974  0.04854063  0.57557416]\n",
      "[-0.06517657 -0.15845063  0.06005212  0.29856938]\n",
      "[-0.06834558  0.03576619  0.0660235   0.025414  ]\n",
      "[-0.06763025 -0.16023742  0.06653178  0.33817565]\n",
      "[-0.070835  -0.3562399  0.0732953  0.6510748]\n",
      "[-0.0779598  -0.16221121  0.08631679  0.38234276]\n",
      "[-0.08120403  0.03158588  0.09396365  0.11807501]\n",
      "[-0.08057231  0.2252447   0.09632514 -0.14354712]\n",
      "[-0.07606742  0.02888467  0.0934542   0.17790335]\n",
      "[-0.07548972 -0.16744179  0.09701227  0.49854353]\n",
      "[-0.07883856  0.02618815  0.10698314  0.2379412 ]\n",
      "[-0.0783148   0.219632    0.11174197 -0.01917253]\n",
      "[-0.07392216  0.41298884  0.11135852 -0.2746151 ]\n",
      "[-0.06566238  0.21646877  0.10586622  0.05101091]\n",
      "[-0.061333    0.02000064  0.10688643  0.37512973]\n",
      "[-0.06093299 -0.17646432  0.11438902  0.69951016]\n",
      "[-0.06446227 -0.3729707   0.12837923  1.0259005 ]\n",
      "[-0.07192169 -0.5695464   0.14889725  1.3559774 ]\n",
      "[-0.08331262 -0.7661896   0.1760168   1.6912926 ]\n",
      "Episode finished after 29 timesteps\n",
      "[0.02929496 0.03353284 0.00873131 0.01968439]\n",
      "[ 0.02996562 -0.16171323  0.009125    0.3151093 ]\n",
      "[0.02673135 0.03327756 0.01542719 0.02531802]\n",
      "[ 0.0273969   0.22817491  0.01593355 -0.26245782]\n",
      "[ 0.0319604   0.42306584  0.01068439 -0.55007285]\n",
      "[ 4.0421717e-02  6.1803609e-01 -3.1706560e-04 -8.3937037e-01]\n",
      "[ 0.05278244  0.42291847 -0.01710447 -0.5467872 ]\n",
      "[ 0.06124081  0.22804096 -0.02804022 -0.25954214]\n",
      "[ 0.06580163  0.42355177 -0.03323106 -0.5609358 ]\n",
      "[ 0.07427266  0.61912394 -0.04444978 -0.86390024]\n",
      "[ 0.08665514  0.8148219  -0.06172778 -1.1702212 ]\n",
      "[ 0.10295158  0.6205546  -0.0851322  -0.8975117 ]\n",
      "[ 0.11536267  0.42668346 -0.10308244 -0.63275677]\n",
      "[ 0.12389635  0.23313917 -0.11573757 -0.374234  ]\n",
      "[ 0.12855913  0.03983513 -0.12322225 -0.1201687 ]\n",
      "[ 0.12935583  0.23648733 -0.12562563 -0.44904613]\n",
      "[ 0.13408558  0.04334535 -0.13460656 -0.19845311]\n",
      "[ 0.13495249 -0.14962025 -0.13857561  0.04892207]\n",
      "[ 0.13196008 -0.3425112  -0.13759717  0.29487386]\n",
      "[ 0.12510985 -0.14572325 -0.1316997  -0.03784415]\n",
      "[ 0.12219539  0.05101727 -0.13245657 -0.3690072 ]\n",
      "[ 0.12321573 -0.14199841 -0.13983671 -0.12084823]\n",
      "[ 0.12037577  0.0548212  -0.14225368 -0.45417398]\n",
      "[ 0.12147219 -0.13803293 -0.15133716 -0.20949648]\n",
      "[ 0.11871153  0.05889248 -0.1555271  -0.5458319 ]\n",
      "[ 0.11988938 -0.1337416  -0.16644374 -0.30590925]\n",
      "[ 0.11721455  0.06331266 -0.17256191 -0.6461146 ]\n",
      "[ 0.1184808   0.26036552 -0.1854842  -0.98778576]\n",
      "[ 0.12368811  0.0681459  -0.20523992 -0.7586222 ]\n",
      "Episode finished after 29 timesteps\n",
      "[ 0.02358434  0.03519122  0.044081   -0.02283172]\n",
      "[ 0.02428817 -0.16053426  0.04362437  0.28342694]\n",
      "[0.02107748 0.0339392  0.0492929  0.00481577]\n",
      "[ 0.02175627  0.22832084  0.04938922 -0.27191627]\n",
      "[0.02632268 0.0325302  0.04395089 0.03592608]\n",
      "[ 0.02697329  0.22699523  0.04466942 -0.24257237]\n",
      "[0.03151319 0.03126462 0.03981797 0.06385911]\n",
      "[ 0.03213848  0.22579373  0.04109515 -0.21599989]\n",
      "[0.03665436 0.03010912 0.03677515 0.08935779]\n",
      "[ 0.03725654  0.22468518  0.03856231 -0.1914995 ]\n",
      "[ 0.04175024  0.41923484  0.03473232 -0.47177255]\n",
      "[ 0.05013494  0.22364     0.02529687 -0.16834795]\n",
      "[0.05460774 0.02816525 0.02192991 0.13220683]\n",
      "[ 0.05517105  0.22296631  0.02457405 -0.15347764]\n",
      "[0.05963037 0.02750128 0.02150449 0.14685544]\n",
      "[ 0.0601804   0.22230878  0.0244416  -0.13896641]\n",
      "[0.06462657 0.02684544 0.02166227 0.16132614]\n",
      "[ 0.06516349 -0.16857982  0.0248888   0.46076345]\n",
      "[ 0.06179189 -0.36404455  0.03410406  0.7611863 ]\n",
      "[ 0.054511   -0.5596193   0.04932779  1.0644025 ]\n",
      "[ 0.04331861 -0.7553582   0.07061584  1.3721503 ]\n",
      "[ 0.02821144 -0.9512886   0.09805885  1.6860571 ]\n",
      "[ 0.00918567 -1.1473988   0.13177998  2.0075924 ]\n",
      "[-0.0137623  -0.95387274  0.17193183  1.7584516 ]\n",
      "[-0.03283976 -0.76106507  0.20710087  1.5238035 ]\n",
      "Episode finished after 25 timesteps\n",
      "[ 0.00971543  0.00713579 -0.02976246  0.03359547]\n",
      "[ 0.00985814 -0.187547   -0.02909055  0.31674144]\n",
      "[ 0.0061072  -0.38224277 -0.02275573  0.6001101 ]\n",
      "[-0.00153765 -0.5770391  -0.01075352  0.88553935]\n",
      "[-0.01307843 -0.7720134   0.00695727  1.1748224 ]\n",
      "[-0.0285187  -0.9672251   0.03045372  1.4696783 ]\n",
      "[-0.0478632  -0.77248865  0.05984728  1.1866611 ]\n",
      "[-0.06331298 -0.9683334   0.0835805   1.497487  ]\n",
      "[-0.08267964 -0.77432084  0.11353024  1.232028  ]\n",
      "[-0.09816606 -0.97070485  0.13817081  1.5580139 ]\n",
      "[-0.11758016 -0.7774814   0.16933107  1.311434  ]\n",
      "[-0.13312979 -0.58485925  0.19555975  1.076179  ]\n",
      "Episode finished after 12 timesteps\n",
      "[ 0.04941954  0.04541678 -0.02692688 -0.0461509 ]\n",
      "[ 0.05032788  0.24091429 -0.02784989 -0.3472064 ]\n",
      "[ 0.05514617  0.43642107 -0.03479402 -0.64853966]\n",
      "[ 0.06387459  0.63201004 -0.04776482 -0.9519729 ]\n",
      "[ 0.07651479  0.43756226 -0.06680427 -0.6746715 ]\n",
      "[ 0.08526603  0.6335459  -0.0802977  -0.987617  ]\n",
      "[ 0.09793695  0.43958557 -0.10005004 -0.7211961 ]\n",
      "[ 0.10672867  0.6359388  -0.11447396 -1.0436188 ]\n",
      "[ 0.11944744  0.44250757 -0.13534634 -0.78895277]\n",
      "[ 0.1282976   0.63920295 -0.1511254  -1.1209681 ]\n",
      "[ 0.14108165  0.8359486  -0.17354475 -1.4569848 ]\n",
      "[ 0.15780063  1.0327228  -0.20268445 -1.7984788 ]\n",
      "Episode finished after 12 timesteps\n"
     ]
    }
   ],
   "source": [
    "for i_episode in range(20):\n",
    "    observation = env.reset()\n",
    "    for t in range(100):\n",
    "        env.render()\n",
    "        print(observation)\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stable Baselines\n",
    "\n",
    "Este algoritmo, DQN, es muy popular, y se encuentra implementado en diferentes librerías. Una de las más interesantes es Stable Baselines 3, que intenta ser el sklearn del aprendizaje por refuerzo. Usa PyTorch internamente. Con Stable Baselines 3, debemos proporcionar un entorno que siga la interfaz Gym y ajustar los hiperparámetros. En el caso de DQN los hiperparámetros más importantes son:\n",
    "\n",
    "- **policy** - La política a usar del modelo. Casi siempre MlpPolicy. Si la entrada son imágenes, CnnPolicy.\n",
    "- **env** - El entorno sobre el que vamos a aprender. Debe implementar la interfaz OpenAI Gym\n",
    "- **learning_rate** - Ratio de aprendizaje de la red neuronal\n",
    "- **buffer_size** - El tamaño del buffer que almacenará las transiciones del \"experience replay\".\n",
    "- **batch_size** - Tamaño del batch que se usa para reentrenar.\n",
    "- **learning_starts** - Cuantos steps debe dar el modelo antes de empezar a aprender la red neuronal.\n",
    "- **gamma** - el factor de descuento. En posts anteriores hemos hablado de él.\n",
    "- **train_freq** - Cada cuantos steps se reentrena el modelo.\n",
    "- **gradient_steps** - Cuantos pasos de gradiente se dan al entrenar. Por defecto, 1.\n",
    "- **target_update_interval** - Cada cuantos steps se actualiza el \"fixed Q-target\".\n",
    "- **policy_kwargs** - Ajustes de la política. En el caso de MlpPolicy podremos ajustar la forma de la red, \n",
    "                    así como las funciones de activación   y más detalles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "env.reset()\n",
    "for _ in range(100):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DQN(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    learning_rate=1e-3,\n",
    "    buffer_size=50000,\n",
    "    learning_starts=10000,\n",
    "    batch_size=64,\n",
    "    gamma=0.999,\n",
    "    gradient_steps=1,\n",
    "    train_freq=20,\n",
    "    target_update_interval=2000,\n",
    "    verbose=1\n",
    "    )\n",
    "model.learn(total_timesteps=500_000)\n",
    "model.save(\"dqn_cartpole\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DQN.load(\"dqn_cartpole\")\n",
    "\n",
    "obs = env.reset()\n",
    "for i in range(1000):\n",
    "    action, _state = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    if done:\n",
    "        obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos con otro ejemplo.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MountainCar\n",
    "\n",
    "![foto7](img/MountainCar.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "for i in range(300):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample()) # accion random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_episode in range(20):\n",
    "    observation = env.reset()\n",
    "    for t in range(100):\n",
    "        env.render()\n",
    "        print(observation)\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --STEP 1--\n",
    "\n",
    "learning = 0.2\n",
    "discount = 0.9\n",
    "epsilon = 0.8\n",
    "min_eps = 0 \n",
    "episodes = 5000\n",
    "\n",
    "# determinar el espacio de estado discretizado\n",
    "\n",
    "num_states = (env.observation_space.high - env.observation_space.low)*np.array([10, 100])\n",
    "num_states = np.round(num_states, 0).astype(int) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --STEP 2--\n",
    "\n",
    "# Recompenzas\n",
    "\n",
    "reward_list = []\n",
    "ave_reward_list = []\n",
    "\n",
    "# Q table\n",
    "\n",
    "Q = np.random.uniform(low = -1, high = 1, size = (num_states[0], num_states[1], env.action_space.n))\n",
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --STEP 3--\n",
    "\n",
    "# parametros (done,state,rewards)\n",
    "\n",
    "done = False\n",
    "tot_reward, reward = 0,0\n",
    "state = env.reset()  \n",
    "        \n",
    "        \n",
    "# Discretizar state\n",
    "\n",
    "state_adj = (state - env.observation_space.low)*np.array([10, 100])\n",
    "state_adj = np.round(state_adj, 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --STEP 4--\n",
    "\n",
    "# Crear un bucle que termine cuando se gane el juego\n",
    "\n",
    "while done != True:   \n",
    "        \n",
    "    env.render()\n",
    "            \n",
    "    # Determinar proxima accion - epsilon greedy strategy\n",
    "    if np.random.random() < 1 - epsilon:\n",
    "           action = np.argmax(Q[state_adj[0], state_adj[1]]) \n",
    "    else:\n",
    "           action = np.random.randint(0, env.action_space.n)\n",
    "                \n",
    "    # Obtener proximo state y reward\n",
    "    state2, reward, done, info = env.step(action)\n",
    "            \n",
    "            \n",
    "    # Discretizar new state\n",
    "    state2_adj = (state2 - env.observation_space.low)*np.array([10, 100])\n",
    "    state2_adj = np.round(state2_adj, 0).astype(int)\n",
    "\n",
    "            \n",
    "    # Permitir estados terminales\n",
    "    if done and state2[0] >= 0.5:\n",
    "          Q[state_adj[0], state_adj[1], action] = reward\n",
    "            \n",
    "        \n",
    "    # Adjustar el valor Q para current state / Aplicar la funcion Q-Learning\n",
    "    else:\n",
    "          Q[state_adj[0], state_adj[1], action] = (1-learning) *Q[state_adj[0], state_adj[1], action] +learning * (reward + discount*Q[state2_adj[0], state2_adj[1],np.argmax(Q[state2_adj[0], state2_adj[1]]) ])\n",
    "    \n",
    "                \n",
    "            \n",
    "              \n",
    "    # Update variables\n",
    "    tot_reward += reward\n",
    "    state_adj = state2_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --STEP 5--\n",
    "\n",
    "reduction = (epsilon - min_eps)/1000\n",
    "\n",
    "for i in range(episodes):\n",
    "\n",
    "    done = False\n",
    "    tot_reward, reward = 0,0\n",
    "    state = env.reset()  \n",
    "        \n",
    "    state_adj = (state - env.observation_space.low)*np.array([10, 100])\n",
    "    state_adj = np.round(state_adj, 0).astype(int)\n",
    "        \n",
    "    while done != True:   \n",
    "        \n",
    "        \n",
    "        if (i+1)%200 == 0:\n",
    "            env.render()\n",
    "            \n",
    "                \n",
    "        if np.random.random() < 1 - epsilon:\n",
    "               action = np.argmax(Q[state_adj[0], state_adj[1]]) \n",
    "        else:\n",
    "               action = np.random.randint(0, env.action_space.n)\n",
    "                \n",
    "        state2, reward, done, info = env.step(action)\n",
    "            \n",
    "            \n",
    "        state2_adj = (state2 - env.observation_space.low)*np.array([10, 100])\n",
    "        state2_adj = np.round(state2_adj, 0).astype(int)\n",
    "\n",
    "            \n",
    "        if done and state2[0] >= 0.5:\n",
    "              Q[state_adj[0], state_adj[1], action] = reward\n",
    "                \n",
    "        else:\n",
    "              Q[state_adj[0], state_adj[1], action] = (1-learning) *Q[state_adj[0], state_adj[1], action] +learning * (reward + discount*Q[state2_adj[0], state2_adj[1],np.argmax(Q[state2_adj[0], state2_adj[1]]) ])\n",
    "    \n",
    "                \n",
    "            \n",
    "              \n",
    "        # Update variables\n",
    "        tot_reward += reward\n",
    "        state_adj = state2_adj\n",
    "        \n",
    "    # Inside the for loop you need to decay epsilon\n",
    "    if epsilon > min_eps:\n",
    "        epsilon -= reduction\n",
    "        \n",
    "    # Track rewards\n",
    "    reward_list.append(tot_reward)\n",
    "        \n",
    "    if (i+1) % 100 == 0:\n",
    "        ave_reward = np.mean(reward_list)\n",
    "        ave_reward_list.append(ave_reward)\n",
    "        print('Episode {} Average Reward: {}'.format(i+1, ave_reward))\n",
    "        reward_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --STEP 6--\n",
    "\n",
    "plt.plot(100*(np.arange(len(ave_reward_list)) + 1), ave_reward_list)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Average Reward')\n",
    "plt.title('Average Reward vs Episodes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otro ejemplo.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taxi\n",
    "\n",
    "![foto8](img/ezgif.com-video-to-gif1.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : :\u001b[43m \u001b[0m|\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Taxi-v3\").env\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay 4 ubicaciones (etiquetadas con letras diferentes), y nuestro trabajo es recoger al pasajero en una ubicación y dejarlo en otra. Recibimos +20 puntos por una entrega exitosa y perdemos 1 punto por cada paso de tiempo que toma. También hay una penalización de 10 puntos por acciones ilegales de recogida y entrega."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space Discrete(6)\n",
      "State Space Discrete(500)\n"
     ]
    }
   ],
   "source": [
    "print(\"Action Space {}\".format(env.action_space))\n",
    "print(\"State Space {}\".format(env.observation_space))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human\n"
     ]
    }
   ],
   "source": [
    "print(env.s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Como se verifica en las impresiones, tenemos un espacio de acción de tamaño 6 y un espacio de estado de tamaño 500. Como verá, nuestro algoritmo RL no necesitará más información que estas dos cosas. Todo lo que necesitamos es una forma de identificar un estado de manera única asignando un número único a cada estado posible, y RL aprende a elegir un número de acción del 0 al 5 donde:\n",
    "\n",
    "- 0 = sur\n",
    "- 1 = norte\n",
    "- 2 = este\n",
    "- 3 = oeste\n",
    "- 4 = recoger\n",
    "- 5 = dejar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.encode(3, 1, 2, 0) # (taxi row, taxi column, passenger index, destination index)\n",
    "print(\"State:\", state)\n",
    "\n",
    "env.s = state\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.s = 328  # set environment to illustration's state\n",
    "\n",
    "epochs = 0\n",
    "penalties, reward = 0, 0\n",
    "\n",
    "frames = [] # for animation\n",
    "\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action)\n",
    "    # Añade +1 a la variable penaltie cuando el reward sea -10\n",
    "    if reward == -10:\n",
    "        penalties = penalties + 1\n",
    "    # Put each rendered frame into dict for animation\n",
    "    frames.append({\n",
    "        'frame': env.render(mode='ansi'),\n",
    "        'state': state,\n",
    "        'action': action,\n",
    "        'reward': reward\n",
    "        }\n",
    "    )\n",
    "\n",
    "    epochs += 1\n",
    "\n",
    "print(\"Timesteps taken: {}\".format(epochs))\n",
    "print(\"Penalties incurred: {}\".format(penalties))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "def print_frames(frames):\n",
    "    for i, frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        print(frame['frame'])\n",
    "        print(f\"Timestep: {i + 1}\")\n",
    "        print(f\"State: {frame['state']}\")\n",
    "        print(f\"Action: {frame['action']}\")\n",
    "        print(f\"Reward: {frame['reward']}\")\n",
    "        sleep(.1)\n",
    "        \n",
    "print_frames(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea un array2d de ceros del tamaño de los diferentes estados y las diferentes posibles acciones\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.uniform(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\"\"\"Training the agent\"\"\"\n",
    "\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.1\n",
    "\n",
    "# For plotting metrics\n",
    "all_epochs = []\n",
    "all_penalties = []\n",
    "\n",
    "for i in range(1, 100001):\n",
    "    state = env.reset()\n",
    "\n",
    "    epochs, penalties, reward, = 0, 0, 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample() # Explore action space\n",
    "        else:\n",
    "            action = np.argmax(q_table[state]) # Exploit learned values\n",
    "\n",
    "        next_state, reward, done, info = env.step(action) \n",
    "        \n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "        \n",
    "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "        q_table[state, action] = new_value\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        state = next_state\n",
    "        epochs += 1\n",
    "        \n",
    "    if i % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Episode: {i}\")\n",
    "\n",
    "print(\"Training finished.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Evaluate agent's performance after Q-learning\"\"\"\n",
    "\n",
    "total_epochs, total_penalties = 0, 0\n",
    "episodes = 100\n",
    "\n",
    "for _ in range(episodes):\n",
    "    env.reset()\n",
    "    # Crea el estado inicial\n",
    "    state = env.encode(3, 1, 2, 0)\n",
    "    env.s = state\n",
    "    # Inicializa las epochs, penalties y rewards\n",
    "    epochs, penalties, reward = 0, 0, 0\n",
    "\n",
    "    done = False\n",
    "    actions = []\n",
    "    while not done:\n",
    "        # Elige la accion que te indique el maximo valor de la q_table\n",
    "        action = np.argmax(q_table[state])\n",
    "        actions.append(action)\n",
    "        # Ejecuta la accion\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        # Actualiza el valor de penalties si el reward es -10\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        epochs += 1\n",
    "\n",
    "    total_penalties += penalties\n",
    "    total_epochs += epochs\n",
    "\n",
    "print(f\"Results after {episodes} episodes:\")\n",
    "print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
    "print(f\"Average penalties per episode: {total_penalties / episodes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Evaluate agent's performance after Q-learning\"\"\"\n",
    "\n",
    "total_epochs, total_penalties = 0, 0\n",
    "episodes = 100\n",
    "\n",
    "for _ in range(episodes):\n",
    "    env.reset()\n",
    "    # Crea el estado inicial\n",
    "    state = env.encode(3, 1, 2, 0)\n",
    "    env.s = state\n",
    "    # Inicializa las epochs, penalties y rewards\n",
    "    epochs, penalties, reward = 0, 0, 0\n",
    "\n",
    "    done = False\n",
    "    actions = []\n",
    "    while not done:\n",
    "        # Elige la accion que te indique el maximo valor de la q_table\n",
    "        action = env.action_space.sample()\n",
    "        actions.append(action)\n",
    "        # Ejecuta la accion\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        # Actualiza el valor de penalties si el reward es -10\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        epochs += 1\n",
    "\n",
    "    total_penalties += penalties\n",
    "    total_epochs += epochs\n",
    "\n",
    "print(f\"Results after {episodes} episodes:\")\n",
    "print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
    "print(f\"Average penalties per episode: {total_penalties / episodes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ahora vamos con algo mas dificil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mario\n",
    "\n",
    "![foto9](img/DDQN-1.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homer le asignamos una tarea?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![foto10](img/homero-homer.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FrozenLake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crear e inicializar el env\n",
    "env = gym.make('FrozenLake-v1', is_slippery=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FH\u001b[41mF\u001b[0mH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "last state = 5\n",
      "reward =  0.0\n",
      "time steps = 12\n"
     ]
    }
   ],
   "source": [
    "is_done = False\n",
    "t = 0\n",
    "while not is_done:\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, is_done, _ = env.step(action)\n",
    "    env.render()\n",
    "    t += 1\n",
    "print('\\nlast state =', state)\n",
    "print('reward = ', reward)\n",
    "print('time steps =', t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, reward, is_done, _ = env.step(action)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "81cc396eb0680590dbaebaf9d83973777df7a169bd63c99e00d2fe00cc20f7dc"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('gym2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
